{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to files\n",
    "path = 'Data/tc4tl_train/data/train/'\n",
    "\n",
    "# Read the .tsv key file\n",
    "df_train_key = pd.read_csv('Data/tc4tl_train/docs/tc4tl_train_key.tsv', sep='\\t', index_col=\"fileid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(path):\n",
    "    # This bit will allow us to cycle through all files in the folder\n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    EDIT - Adapted from the first part of Owen's code\n",
    "    Changed list to dictionary so that file name would be stored also\n",
    "        - might be easier to then link with the key files?\n",
    "        - \"li\" changed to \"dic\"\n",
    "        \n",
    "    Output should be a dictionary where each key value pair is as follows\n",
    "        - key = file name\n",
    "        - value = dataframe with bluetooth, accelerometer data etc\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an empty list\n",
    "    dic = {}\n",
    "    # Read in each file from the folder and add them to the list\n",
    "    # WARNING This may not read in the CSVs in the order they were in in the folder\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, header=None, skiprows=7, sep='\\n', skip_blank_lines=False, quoting=csv.QUOTE_NONE)\n",
    "        df = df[0].str.split(',', expand=True)\n",
    "        dic[filename[len(path):]] = df\n",
    "    \n",
    "        \n",
    "    return dic\n",
    "\n",
    "dic = get_dict(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- TAKES A WHILE TO RUN\n",
    "\"\"\"\n",
    "For best run go back to min-max rather than quartiles and remove gyro_2\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "This function will create a dataframe from the key tsv file provided by NIST\n",
    "\n",
    "Then it will add a column with the average bluetooth, accelerometer etc. for each file\n",
    "\n",
    "It also gets dummy variables for the coarse_grain variable\n",
    "\"\"\"\n",
    "\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "def add_predictor_aggregated(dic, key_df):\n",
    "    key_df = key_df.copy()\n",
    "    \"\"\"\n",
    "    dic - output of the get_dict() function\n",
    "    key_df - the dataframe obtained from reading the .tsv file provided by nist\n",
    "    \"\"\"\n",
    "    # Get rough summary statistics for each file to create a baseline model and append to key dataframe\n",
    "    key_df[\"bluetooth_avg\"] = np.nan\n",
    "    key_df[\"bluetooth_lwr\"] = np.nan\n",
    "    key_df[\"bluetooth_upr\"] = np.nan\n",
    "    key_df[\"bluetooth_std\"] = np.nan\n",
    "    key_df[\"bluetooth_skew\"] = np.nan\n",
    "    key_df[\"accelerometer_1_avg\"] = np.nan\n",
    "    key_df[\"accelerometer_1_lwr\"] = np.nan\n",
    "    key_df[\"accelerometer_1_upr\"] = np.nan\n",
    "    key_df[\"accelerometer_1_std\"] = np.nan\n",
    "    key_df[\"accelerometer_1_skew\"] = np.nan\n",
    "    key_df[\"accelerometer_2_avg\"] = np.nan\n",
    "    key_df[\"accelerometer_2_lwr\"] = np.nan\n",
    "    key_df[\"accelerometer_2_upr\"] = np.nan\n",
    "    key_df[\"accelerometer_2_std\"] = np.nan\n",
    "    key_df[\"accelerometer_2_skew\"] = np.nan\n",
    "    key_df[\"gyro_1_avg\"] = np.nan\n",
    "    key_df[\"gyro_1_lwr\"] = np.nan\n",
    "    key_df[\"gyro_1_upr\"] = np.nan\n",
    "    key_df[\"gyro_1_std\"] = np.nan\n",
    "    key_df[\"gyro_2_avg\"] = np.nan\n",
    "    key_df[\"gyro_2_lwr\"] = np.nan\n",
    "    key_df[\"gyro_2_upr\"] = np.nan\n",
    "    key_df[\"gyro_2_std\"] = np.nan\n",
    "    key_df[\"gyro_3_avg\"] = np.nan\n",
    "    key_df[\"gyro_3_lwr\"] = np.nan\n",
    "    key_df[\"gyro_3_upr\"] = np.nan\n",
    "    key_df[\"gyro_3_std\"] = np.nan\n",
    "    #key_df[\"attitude_avg\"] = np.nan\n",
    "    #key_df[\"gravity_avg\"] = np.nan\n",
    "\n",
    "    for file_id, value in dic.items():\n",
    "        bt_temp = value[value.iloc[:,1]==\"Bluetooth\"].iloc[:,2].astype(float)\n",
    "        bt_mean = np.mean(bt_temp)\n",
    "        bt_lwr = np.quantile(bt_temp, 0.1)\n",
    "        bt_upr = np.quantile(bt_temp, 0.9)\n",
    "        bt_std = np.std(bt_temp)\n",
    "        bt_skew = skew(bt_temp)\n",
    "        \n",
    "        acc_1_temp = value[value.iloc[:,1]==\"Accelerometer\"].iloc[:,2].astype(float)\n",
    "        accelerometer_mean = np.mean(acc_1_temp)\n",
    "        accelerometer_lwr = np.quantile(acc_1_temp, 0.1)\n",
    "        accelerometer_upr = np.quantile(acc_1_temp, 0.9)\n",
    "        accelerometer_std = np.std(acc_1_temp)\n",
    "        accelerometer_skew = skew(acc_1_temp)\n",
    "        \n",
    "        acc_2_temp = value[value.iloc[:,1]==\"Accelerometer\"].iloc[:,3].astype(float)\n",
    "        accelerometer_2_mean = np.mean(acc_2_temp)\n",
    "        accelerometer_2_lwr = np.quantile(acc_2_temp, 0.1)\n",
    "        accelerometer_2_upr = np.quantile(acc_2_temp, 0.9)\n",
    "        accelerometer_2_std = np.std(acc_2_temp)\n",
    "        accelerometer_2_skew = skew(acc_2_temp)\n",
    "        \n",
    "        gyro_1_temp = value[value.iloc[:,1]==\"Gyroscope\"].iloc[:,2].astype(float)\n",
    "        gyro_1_mean = np.mean(gyro_1_temp)\n",
    "        gyro_1_lwr = np.quantile(gyro_1_temp, 0.1)\n",
    "        gyro_1_upr = np.quantile(gyro_1_temp, 0.9)\n",
    "        gyro_1_std = np.std(gyro_1_temp)\n",
    "        \n",
    "        gyro_2_temp = value[value.iloc[:,1]==\"Gyroscope\"].iloc[:,3].astype(float)\n",
    "        gyro_2_mean = np.mean(gyro_2_temp)\n",
    "        gyro_2_lwr = np.quantile(gyro_2_temp, 0.1)\n",
    "        gyro_2_upr = np.quantile(gyro_2_temp, 0.9)\n",
    "        gyro_2_std = np.std(gyro_2_temp)\n",
    "        \n",
    "        gyro_3_temp = value[value.iloc[:,1]==\"Gyroscope\"].iloc[:,4].astype(float)\n",
    "        gyro_3_mean = np.mean(gyro_3_temp)\n",
    "        gyro_3_lwr = np.quantile(gyro_3_temp, 0.1)\n",
    "        gyro_3_upr = np.quantile(gyro_3_temp, 0.9)\n",
    "        gyro_3_std = np.std(gyro_3_temp)\n",
    "        \n",
    "        #attitude_mean = np.mean(value[value.iloc[:,1]==\"Attitude\"].iloc[:,2].astype(float))\n",
    "        #gravity_mean = np.mean(value[value.iloc[:,1]==\"Gravity\"].iloc[:,2].astype(float))\n",
    "\n",
    "        key_df.loc[file_id, \"bluetooth_avg\"] = bt_mean\n",
    "        key_df.loc[file_id, \"bluetooth_lwr\"] = bt_lwr\n",
    "        key_df.loc[file_id, \"bluetooth_upr\"] = bt_upr\n",
    "        key_df.loc[file_id, \"bluetooth_std\"] = bt_std\n",
    "        key_df.loc[file_id, \"bluetooth_skew\"] = bt_skew\n",
    "        \n",
    "        key_df.loc[file_id, \"accelerometer_1_avg\"] = accelerometer_mean\n",
    "        key_df.loc[file_id, \"accelerometer_1_lwr\"] = accelerometer_lwr\n",
    "        key_df.loc[file_id, \"accelerometer_1_upr\"] = accelerometer_upr\n",
    "        key_df.loc[file_id, \"accelerometer_1_std\"] = accelerometer_std\n",
    "        key_df.loc[file_id, \"accelerometer_1_skew\"] = accelerometer_skew\n",
    "        \n",
    "        key_df.loc[file_id, \"accelerometer_2_avg\"] = accelerometer_2_mean\n",
    "        key_df.loc[file_id, \"accelerometer_2_lwr\"] = accelerometer_2_lwr\n",
    "        key_df.loc[file_id, \"accelerometer_2_upr\"] = accelerometer_2_upr\n",
    "        key_df.loc[file_id, \"accelerometer_2_std\"] = accelerometer_2_std\n",
    "        key_df.loc[file_id, \"accelerometer_2_skew\"] = accelerometer_2_skew\n",
    "        \n",
    "        key_df.loc[file_id, \"gyro_1_avg\"] = gyro_1_mean\n",
    "        key_df.loc[file_id, \"gyro_1_lwr\"] = gyro_1_lwr\n",
    "        key_df.loc[file_id, \"gyro_1_upr\"] = gyro_1_upr\n",
    "        key_df.loc[file_id, \"gyro_1_std\"] = gyro_1_std\n",
    "        \n",
    "        key_df.loc[file_id, \"gyro_2_avg\"] = gyro_2_mean\n",
    "        key_df.loc[file_id, \"gyro_2_lwr\"] = gyro_2_lwr\n",
    "        key_df.loc[file_id, \"gyro_2_upr\"] = gyro_2_upr\n",
    "        key_df.loc[file_id, \"gyro_2_std\"] = gyro_2_std\n",
    "        \n",
    "        key_df.loc[file_id, \"gyro_3_avg\"] = gyro_3_mean\n",
    "        key_df.loc[file_id, \"gyro_3_lwr\"] = gyro_3_lwr\n",
    "        key_df.loc[file_id, \"gyro_3_upr\"] = gyro_3_upr\n",
    "        key_df.loc[file_id, \"gyro_3_std\"] = gyro_3_std\n",
    "        \n",
    "        #key_df.loc[file_id, \"attitude_avg\"] = attitude_mean\n",
    "        #key_df.loc[file_id, \"gravity_avg\"] = gravity_mean\n",
    "        \n",
    "    #key_df = pd.get_dummies(key_df, columns=['coarse_grain'])\n",
    "        \n",
    "    return key_df\n",
    "\n",
    "key_df = add_predictor_aggregated(dic, df_train_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['phone_carriage_state', 'distance_in_meters', 'step_size_in_sec',\n",
       "       'coarse_grain', 'bluetooth_avg', 'bluetooth_lwr', 'bluetooth_upr',\n",
       "       'bluetooth_std', 'bluetooth_skew', 'accelerometer_1_avg',\n",
       "       'accelerometer_1_lwr', 'accelerometer_1_upr', 'accelerometer_1_std',\n",
       "       'accelerometer_1_skew', 'accelerometer_2_avg', 'accelerometer_2_lwr',\n",
       "       'accelerometer_2_upr', 'accelerometer_2_std', 'accelerometer_2_skew',\n",
       "       'gyro_1_avg', 'gyro_1_lwr', 'gyro_1_upr', 'gyro_1_std', 'gyro_2_avg',\n",
       "       'gyro_2_lwr', 'gyro_2_upr', 'gyro_2_std', 'gyro_3_avg', 'gyro_3_lwr',\n",
       "       'gyro_3_upr', 'gyro_3_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone_carriage_state</th>\n",
       "      <th>distance_in_meters</th>\n",
       "      <th>step_size_in_sec</th>\n",
       "      <th>bluetooth_avg</th>\n",
       "      <th>bluetooth_lwr</th>\n",
       "      <th>bluetooth_upr</th>\n",
       "      <th>bluetooth_std</th>\n",
       "      <th>accelerometer_avg</th>\n",
       "      <th>accelerometer_lwr</th>\n",
       "      <th>accelerometer_upr</th>\n",
       "      <th>accelerometer_std</th>\n",
       "      <th>gyroscope_avg</th>\n",
       "      <th>gyroscope_avg_2</th>\n",
       "      <th>attitude_avg</th>\n",
       "      <th>gravity_avg</th>\n",
       "      <th>gyroscope_avg_3</th>\n",
       "      <th>coarse_grain_N</th>\n",
       "      <th>coarse_grain_Y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fileid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaadbuvp_tc4tl20.csv</th>\n",
       "      <td>pocket_hand</td>\n",
       "      <td>1.8</td>\n",
       "      <td>50</td>\n",
       "      <td>-62.133482</td>\n",
       "      <td>-74.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>7.573810</td>\n",
       "      <td>0.024843</td>\n",
       "      <td>-0.040276</td>\n",
       "      <td>0.051270</td>\n",
       "      <td>0.051270</td>\n",
       "      <td>0.007361</td>\n",
       "      <td>-0.001176</td>\n",
       "      <td>0.726321</td>\n",
       "      <td>0.025127</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaamkcii_tc4tl20.csv</th>\n",
       "      <td>pocket_pocket</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80</td>\n",
       "      <td>-55.255556</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>5.254757</td>\n",
       "      <td>0.187249</td>\n",
       "      <td>-0.218625</td>\n",
       "      <td>0.444652</td>\n",
       "      <td>0.444652</td>\n",
       "      <td>-0.017030</td>\n",
       "      <td>-0.019209</td>\n",
       "      <td>-0.410232</td>\n",
       "      <td>0.186804</td>\n",
       "      <td>0.015339</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aabqtowt_tc4tl20.csv</th>\n",
       "      <td>hand_hand</td>\n",
       "      <td>1.2</td>\n",
       "      <td>60</td>\n",
       "      <td>-62.289089</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>8.674381</td>\n",
       "      <td>0.050939</td>\n",
       "      <td>0.024933</td>\n",
       "      <td>0.077966</td>\n",
       "      <td>0.077966</td>\n",
       "      <td>0.007156</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>0.954430</td>\n",
       "      <td>0.050841</td>\n",
       "      <td>-0.006214</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aadkjwss_tc4tl20.csv</th>\n",
       "      <td>pocket_pocket</td>\n",
       "      <td>1.8</td>\n",
       "      <td>20</td>\n",
       "      <td>-67.938259</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>6.982572</td>\n",
       "      <td>-0.067156</td>\n",
       "      <td>-0.157225</td>\n",
       "      <td>0.111467</td>\n",
       "      <td>0.111467</td>\n",
       "      <td>0.028015</td>\n",
       "      <td>0.053077</td>\n",
       "      <td>1.337362</td>\n",
       "      <td>-0.069499</td>\n",
       "      <td>0.044304</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aafzrgzt_tc4tl20.csv</th>\n",
       "      <td>hand_hand</td>\n",
       "      <td>1.8</td>\n",
       "      <td>60</td>\n",
       "      <td>-55.760797</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>5.289863</td>\n",
       "      <td>0.015989</td>\n",
       "      <td>-0.047058</td>\n",
       "      <td>0.087917</td>\n",
       "      <td>0.087917</td>\n",
       "      <td>-0.007063</td>\n",
       "      <td>-0.006210</td>\n",
       "      <td>0.292535</td>\n",
       "      <td>0.016077</td>\n",
       "      <td>-0.008596</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     phone_carriage_state  distance_in_meters  \\\n",
       "fileid                                                          \n",
       "aaadbuvp_tc4tl20.csv          pocket_hand                 1.8   \n",
       "aaamkcii_tc4tl20.csv        pocket_pocket                 3.0   \n",
       "aabqtowt_tc4tl20.csv            hand_hand                 1.2   \n",
       "aadkjwss_tc4tl20.csv        pocket_pocket                 1.8   \n",
       "aafzrgzt_tc4tl20.csv            hand_hand                 1.8   \n",
       "\n",
       "                      step_size_in_sec  bluetooth_avg  bluetooth_lwr  \\\n",
       "fileid                                                                 \n",
       "aaadbuvp_tc4tl20.csv                50     -62.133482          -74.0   \n",
       "aaamkcii_tc4tl20.csv                80     -55.255556          -60.0   \n",
       "aabqtowt_tc4tl20.csv                60     -62.289089          -75.0   \n",
       "aadkjwss_tc4tl20.csv                20     -67.938259          -78.0   \n",
       "aafzrgzt_tc4tl20.csv                60     -55.760797          -63.0   \n",
       "\n",
       "                      bluetooth_upr  bluetooth_std  accelerometer_avg  \\\n",
       "fileid                                                                  \n",
       "aaadbuvp_tc4tl20.csv          -52.0       7.573810           0.024843   \n",
       "aaamkcii_tc4tl20.csv          -49.0       5.254757           0.187249   \n",
       "aabqtowt_tc4tl20.csv          -52.0       8.674381           0.050939   \n",
       "aadkjwss_tc4tl20.csv          -61.0       6.982572          -0.067156   \n",
       "aafzrgzt_tc4tl20.csv          -48.0       5.289863           0.015989   \n",
       "\n",
       "                      accelerometer_lwr  accelerometer_upr  accelerometer_std  \\\n",
       "fileid                                                                          \n",
       "aaadbuvp_tc4tl20.csv          -0.040276           0.051270           0.051270   \n",
       "aaamkcii_tc4tl20.csv          -0.218625           0.444652           0.444652   \n",
       "aabqtowt_tc4tl20.csv           0.024933           0.077966           0.077966   \n",
       "aadkjwss_tc4tl20.csv          -0.157225           0.111467           0.111467   \n",
       "aafzrgzt_tc4tl20.csv          -0.047058           0.087917           0.087917   \n",
       "\n",
       "                      gyroscope_avg  gyroscope_avg_2  attitude_avg  \\\n",
       "fileid                                                               \n",
       "aaadbuvp_tc4tl20.csv       0.007361        -0.001176      0.726321   \n",
       "aaamkcii_tc4tl20.csv      -0.017030        -0.019209     -0.410232   \n",
       "aabqtowt_tc4tl20.csv       0.007156        -0.005191      0.954430   \n",
       "aadkjwss_tc4tl20.csv       0.028015         0.053077      1.337362   \n",
       "aafzrgzt_tc4tl20.csv      -0.007063        -0.006210      0.292535   \n",
       "\n",
       "                      gravity_avg  gyroscope_avg_3  coarse_grain_N  \\\n",
       "fileid                                                               \n",
       "aaadbuvp_tc4tl20.csv     0.025127         0.009136               0   \n",
       "aaamkcii_tc4tl20.csv     0.186804         0.015339               1   \n",
       "aabqtowt_tc4tl20.csv     0.050841        -0.006214               1   \n",
       "aadkjwss_tc4tl20.csv    -0.069499         0.044304               1   \n",
       "aafzrgzt_tc4tl20.csv     0.016077        -0.008596               0   \n",
       "\n",
       "                      coarse_grain_Y  \n",
       "fileid                                \n",
       "aaadbuvp_tc4tl20.csv               1  \n",
       "aaamkcii_tc4tl20.csv               0  \n",
       "aabqtowt_tc4tl20.csv               0  \n",
       "aadkjwss_tc4tl20.csv               0  \n",
       "aafzrgzt_tc4tl20.csv               1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key_df.to_csv('aggregated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Take just variables we want\n",
    "#-- No Skew\n",
    "predictor_columns = ['bluetooth_avg', 'bluetooth_lwr', 'bluetooth_upr',\n",
    "                     'bluetooth_std', 'accelerometer_1_avg', 'accelerometer_1_lwr',\n",
    "                     'accelerometer_1_upr', 'accelerometer_1_std', 'accelerometer_2_avg',\n",
    "                     'accelerometer_2_lwr', 'accelerometer_2_upr', 'accelerometer_2_std',\n",
    "                     'gyro_1_avg', 'gyro_1_lwr', 'gyro_1_upr', 'gyro_1_std', 'gyro_2_avg',\n",
    "                     'gyro_2_lwr', 'gyro_2_upr', 'gyro_2_std', 'gyro_3_avg', 'gyro_3_lwr',\n",
    "                     'gyro_3_upr', 'gyro_3_std']\n",
    "\n",
    "#-- Skew\n",
    "\"\"\"predictor_columns = ['bluetooth_avg', 'bluetooth_lwr', 'bluetooth_upr',\n",
    "       'bluetooth_std', 'bluetooth_skew', 'accelerometer_1_avg',\n",
    "       'accelerometer_1_lwr', 'accelerometer_1_upr', 'accelerometer_1_std',\n",
    "       'accelerometer_1_skew', 'accelerometer_2_avg', 'accelerometer_2_lwr',\n",
    "       'accelerometer_2_upr', 'accelerometer_2_std', 'accelerometer_2_skew',\n",
    "       'gyro_1_avg', 'gyro_1_lwr', 'gyro_1_upr', 'gyro_1_std', 'gyro_2_avg',\n",
    "       'gyro_2_lwr', 'gyro_2_upr', 'gyro_2_std', 'gyro_3_avg', 'gyro_3_lwr',\n",
    "       'gyro_3_upr', 'gyro_3_std']\"\"\"\n",
    "\n",
    "\n",
    "X = key_df[predictor_columns]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "y = key_df[\"distance_in_meters\"].apply(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rian/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\"\"\"\n",
    "Fit all of the imported models and store predictions\n",
    "\"\"\"\n",
    "\n",
    "model1 = RandomForestClassifier()\n",
    "model1.fit(X_train, y_train)\n",
    "predictions1 = model1.predict(X_test)\n",
    "\n",
    "model2 = KNeighborsClassifier()\n",
    "model2.fit(X_train, y_train)\n",
    "predictions2 = model2.predict(X_test)\n",
    "\n",
    "model3 = AdaBoostClassifier()\n",
    "model3.fit(X_train, y_train)\n",
    "predictions3 = model3.predict(X_test)\n",
    "\n",
    "model4 = GradientBoostingClassifier()\n",
    "model4.fit(X_train, y_train)\n",
    "predictions4 = model4.predict(X_test)\n",
    "\n",
    "model5 = MLPClassifier()\n",
    "model5.fit(X_train, y_train)\n",
    "predictions5 = model5.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Random Forest :  528.9300000000001\n",
      "MSE for K Neighbors :  1497.2400000000002\n",
      "MSE for Ada Boost :  4862.160000000001\n",
      "MSE for Gradient Boosting :  2392.7400000000007\n",
      "MSE for Multi Layer Perceptron :  2486.07\n"
     ]
    }
   ],
   "source": [
    "#-- Print the MSE for each model\n",
    "print(\"MSE for Random Forest : \", np.sum((predictions1.astype(float)-y_test.astype(float))**2))\n",
    "print(\"MSE for K Neighbors : \", np.sum((predictions2.astype(float)-y_test.astype(float))**2))\n",
    "print(\"MSE for Ada Boost : \", np.sum((predictions3.astype(float)-y_test.astype(float))**2))\n",
    "print(\"MSE for Gradient Boosting : \", np.sum((predictions4.astype(float)-y_test.astype(float))**2))\n",
    "print(\"MSE for Multi Layer Perceptron : \", np.sum((predictions5.astype(float)-y_test.astype(float))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3. , 4.5, 4.5, ..., 1.8, 1.2, 3. ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileid\n",
       "yqjxjjcj_tc4tl20.csv    3.0\n",
       "oetvnfyd_tc4tl20.csv    4.5\n",
       "owilsukc_tc4tl20.csv    4.5\n",
       "dhzcxlwg_tc4tl20.csv    4.5\n",
       "butaetck_tc4tl20.csv    1.8\n",
       "                       ... \n",
       "ahpvtsni_tc4tl20.csv    1.8\n",
       "crbdzqme_tc4tl20.csv    1.2\n",
       "bbkbjbed_tc4tl20.csv    1.8\n",
       "afyiwipu_tc4tl20.csv    1.2\n",
       "vibvnhbe_tc4tl20.csv    3.0\n",
       "Name: distance_in_meters, Length: 5133, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True values for rough comparison\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to competition Dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to files\n",
    "dev_path = 'Data/tc4tl_dev_test/data/dev/'\n",
    "\n",
    "# Read the .tsv key file\n",
    "df_dev_key = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_dev_key.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "dev_dict = get_dict(dev_path)\n",
    "\n",
    "dev_key_df = add_predictor_aggregated(dev_dict, df_dev_key)\n",
    "\n",
    "def predict_from_key_df(key_df, model, scaler):\n",
    "    X = key_df[predictor_columns]\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    return model.predict(X)\n",
    "\n",
    "#-- !!!!!!! CHANGE model4 below to whatever model you want to get the output file for!\n",
    "dev_predictions = predict_from_key_df(dev_key_df, model1, scaler)\n",
    "\n",
    "df_dev_trials = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_dev_trials.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "df_dev_trials['distance'] = dev_predictions.astype(float)\n",
    "\n",
    "df_dev_trials.to_csv('dev_system_output_RF.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fileid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abgikaek_tc4tl20.csv</th>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acehqsss_tc4tl20.csv</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adcmsfnp_tc4tl20.csv</th>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adljjzjj_tc4tl20.csv</th>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adzvqmmg_tc4tl20.csv</th>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      distance\n",
       "fileid                        \n",
       "abgikaek_tc4tl20.csv       1.2\n",
       "acehqsss_tc4tl20.csv       3.0\n",
       "adcmsfnp_tc4tl20.csv       1.8\n",
       "adljjzjj_tc4tl20.csv       1.2\n",
       "adzvqmmg_tc4tl20.csv       4.5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_trials.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to files\n",
    "dev_path = 'Data/tc4tl_dev_test/data/test/'\n",
    "\n",
    "# Read the .tsv key file\n",
    "df_dev_key = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_test_metadata.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "dev_dict = get_dict(dev_path)\n",
    "\n",
    "dev_key_df = add_predictor_aggregated(dev_dict, df_dev_key)\n",
    "\n",
    "def predict_from_key_df(key_df, model, scaler):\n",
    "    X = key_df[predictor_columns]\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    return model.predict(X)\n",
    "\n",
    "#-- !!!!!!! CHANGE model4 below to whatever model you want to get the output file for!\n",
    "dev_predictions = predict_from_key_df(dev_key_df, model1, scaler)\n",
    "\n",
    "df_dev_trials = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_test_trials.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "df_dev_trials['distance'] = dev_predictions.astype(float)\n",
    "\n",
    "df_dev_trials.to_csv('test_system_output_RF_no_skew.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to files\n",
    "dev_path = 'Data/tc4tl_dev_test/data/test/'\n",
    "\n",
    "# Read the .tsv key file\n",
    "df_dev_key = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_test_metadata.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "dev_dict = get_dict(dev_path)\n",
    "\n",
    "dev_key_df = add_predictor_aggregated(dev_dict, df_dev_key)\n",
    "\n",
    "def predict_from_key_df(key_df, model, scaler):\n",
    "    X = key_df[predictor_columns]\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    return model.predict(X)\n",
    "\n",
    "#-- !!!!!!! CHANGE model4 below to whatever model you want to get the output file for!\n",
    "dev_predictions = predict_from_key_df(dev_key_df, model2, scaler)\n",
    "\n",
    "df_dev_trials = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_test_trials.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "df_dev_trials['distance'] = dev_predictions.astype(float)\n",
    "\n",
    "df_dev_trials.to_csv('test_system_output_KNN_no_skew.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to files\n",
    "dev_path = 'Data/tc4tl_dev_test/data/test/'\n",
    "\n",
    "# Read the .tsv key file\n",
    "df_dev_key = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_test_metadata.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "dev_dict = get_dict(dev_path)\n",
    "\n",
    "dev_key_df = add_predictor_aggregated(dev_dict, df_dev_key)\n",
    "\n",
    "def predict_from_key_df(key_df, model, scaler):\n",
    "    X = key_df[predictor_columns]\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    return model.predict(X)\n",
    "\n",
    "#-- !!!!!!! CHANGE model4 below to whatever model you want to get the output file for!\n",
    "dev_predictions = predict_from_key_df(dev_key_df, model3, scaler)\n",
    "\n",
    "df_dev_trials = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_test_trials.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "df_dev_trials['distance'] = dev_predictions.astype(float)\n",
    "\n",
    "df_dev_trials.to_csv('test_system_output_Ada_no_skew.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to files\n",
    "dev_path = 'Data/tc4tl_dev_test/data/test/'\n",
    "\n",
    "# Read the .tsv key file\n",
    "df_dev_key = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_test_metadata.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "dev_dict = get_dict(dev_path)\n",
    "\n",
    "dev_key_df = add_predictor_aggregated(dev_dict, df_dev_key)\n",
    "\n",
    "def predict_from_key_df(key_df, model, scaler):\n",
    "    X = key_df[predictor_columns]\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    return model.predict(X)\n",
    "\n",
    "#-- !!!!!!! CHANGE model4 below to whatever model you want to get the output file for!\n",
    "dev_predictions = predict_from_key_df(dev_key_df, model4, scaler)\n",
    "\n",
    "df_dev_trials = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_test_trials.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "df_dev_trials['distance'] = dev_predictions.astype(float)\n",
    "\n",
    "df_dev_trials.to_csv('test_system_output_XGB_no_skew.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to files\n",
    "dev_path = 'Data/tc4tl_dev_test/data/test/'\n",
    "\n",
    "# Read the .tsv key file\n",
    "df_dev_key = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_test_metadata.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "dev_dict = get_dict(dev_path)\n",
    "\n",
    "dev_key_df = add_predictor_aggregated(dev_dict, df_dev_key)\n",
    "\n",
    "def predict_from_key_df(key_df, model, scaler):\n",
    "    X = key_df[predictor_columns]\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    return model.predict(X)\n",
    "\n",
    "#-- !!!!!!! CHANGE model4 below to whatever model you want to get the output file for!\n",
    "dev_predictions = predict_from_key_df(dev_key_df, model5, scaler)\n",
    "\n",
    "df_dev_trials = pd.read_csv('Data/tc4tl_dev_test/docs/tc4tl_test_trials.tsv', sep='\\t', index_col=\"fileid\")\n",
    "\n",
    "df_dev_trials['distance'] = dev_predictions.astype(float)\n",
    "\n",
    "df_dev_trials.to_csv('test_system_output_MLP_no_skew.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
